<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Wenhai Wang</title>
    <meta content="Wenhai Wang, https://whai362.github.io" name="keywords">
    <link rel="stylesheet" type="text/css" href="resources/css/mystyle.css">
    <link rel="stylesheet" type="text/css" href="resources/css/font.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-164510176-1');
    </script>
</head>

<body>
    <!-- <iframe class="info" scrolling="no"  src="resources/html/info.html"
        style="border: 1px solid #ddd; margin-bottom: 1em; padding: 1em; background-color: #fff;"></iframe> -->
    <div class="section">
        <div class="paper">
            <img class="info" title="whai362" style="float: left; padding-left: .01em;" src="resources/images/me.png">
            <!-- <img class="info" title="affiliation" style="float: right; padding-right: .01em;" src="resources/images/affiliation1.png"> -->
            <div class="info" style="padding-left: 11.5em; vertical-align: top;">
                <span style="line-height: 150%; font-size: 25pt;">Wenhai Wang (王文海)</span><br>
                <strong>Affiliation</strong>: Fundamental Vision Department, <a href="https://www.shlab.org.cn/">Shanghai AI
                        Laboratory</a><br>
                <strong>Address</strong>: 701 Yunjin Road, Xuhui District, Shanghai, China<br>
                <strong>Email</strong>: wangwenhai362[at]{163.com, gmail.com} wangwenhai[at]pjlab.org.cn<br>
            </div>
            <div class="spanner"></div>
        </div>
    </div>

    <div class="section">
        <h2>About Me (<a href="https://github.com/whai362">[GitHub]</a>
            <a href="https://scholar.google.com/citations?user=WM0OglcAAAAJ">[Google Scholar]</a>
            <a href="resources/cv/wangwenhai_cv.pdf">[CV]</a>)
        </h2>
        <div class="paper">
            I am a Research Scientist at <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a>, collaborated with
            <a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ">Prof. Jifeng Dai</a> and <a
                href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ">Prof. Yu Qiao</a>
            <br><br>

            Previously, I obtained the Ph.D. degree from Department of Computer Science and Technology, Nanjing
            University (NJU) in 2021.
            My academic supervisor is <a href="https://cs.nju.edu.cn/lutong/">Prof. Tong Lu</a>.
            I received my B.E degree from Nanjing University of Science and Technology (NUST) in 2016.
            <br>

            I work very close with my friends <a href="https://scholar.google.com/citations?user=42MVVPgAAAAJ">Dr. Enze
                Xie</a> and <a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ">Prof. Xiang Li</a>.

            I was fortunate to work with <a href="https://scholar.google.com.hk/citations?user=aXdjxb4AAAAJ">Prof.
                Ping Luo</a> and <a href="https://scholar.google.com/citations?user=Ljk2BvIAAAAJ">Prof. Chunhua
                Shen</a>.

            <br><br>
            My recent works are mainly on:
            <ul>
                <li>CNN/Transformer Backbone</li>
                <li>Object Detection & Semantic/Instance/Panoptic Segmentation</li>
                <li>Vision-Language Model</li>
                <li>Autonomous Driving Perception</li>
                <li>Optical Character Recognition</li>
            </ul>
            <br>
            <alert>The fundamental vision department at <a href="https://www.shlab.org.cn/">Shanghai AI
                    Laboratory</a> is now hiring.
                If you are interested in internship/researcher positions related to computer vision, please feel
                free to contact me through the email.
            </alert>
            <!-- <br><p style='color:red'><strong>I am looking for a postdoctoral position or a full-time job. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>



    <div class="section">
        <h2 id="news">News</h2>
        <div class="paper">
            <ul>
                <li>
                    2023/03: <a
                        href="https://arxiv.org/abs/2212.10156">UniAD</a> is selected as one of CVPR 2023 award candidates (12 out of 9155)</a>.
                </li>
                <li>
                    2023/01: <a
                        href="https://arxiv.org/pdf/2203.17270.pdf">BEVFormer</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/">ECCV 2022' Top-10
                        Influential Papers</a> and <a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">the 100 most cited AI papers in 2022</a>.
                </li>
                <li>
                    2023/01: <a href="https://link.springer.com/article/10.1007/s41095-022-0274-8">PVT v2</a> is selected as one of <a href="https://mp.weixin.qq.com/s/xdrKpK2dCShWUWmISYM6Yw">ESI highly cited paper (1%)</a> and <a href="https://mp.weixin.qq.com/s/xdrKpK2dCShWUWmISYM6Yw">ESI hot papers (0.1%)</a>.
                </li>
                <li>
                    2022/11: We release <a href="https://arxiv.org/abs/2211.05778.pdf">InternImage</a>, setting a new record <alert>65.4 box mAP</alert> on COCO test-dev.
                </li>
                <li>
                    2022/06: Our team wins the
                    champion of <a href="https://waymo.com/open/challenges/2022/3d-camera-only-detection/">Waymo 2022 3D Camera-Only Detection Task (15,000 USD Bonus)</a>.
                </li>
                <li>
                    2022/04: I am selected as one of <a
                        href="https://mp.weixin.qq.com/s/GfeYJ6zpGYaVHs2kKpjQcA">TechBeat 2022 Most Popular
                        Speakers</a>.
                </li>
                <li>
                    2022/02: <a
                        href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf">PVT</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/">ICCV21'
                        Top-10 Influential Papers (Rank 2)</a>,
                    and <a
                        href="https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf">SegFormer</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/">NeurIPS21'
                        Top-10 Influential Papers (Rank 3)</a>.
                </li>
                <li>
                    2021/02: <a
                        href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_PolarMask_Single_Shot_Instance_Segmentation_With_Polar_Representation_CVPR_2020_paper.pdf">PolarMask</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/">CVPR20' Top-10
                        Influential Papers</a>.
                </li>
                <li>
                    2020/12: Our team wins the champion of <a href="https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm">NAIC 2020 Remote Sensing Semantic Segmentation Task (1,000,000 RMB bonus).</a>
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>



    <div class="section">
        <h2 id="experience">Experience</h2>
        <div class="paper">
            <ul>
                <li>
                    2021/09 - Present: Research Scientist at Shanghai AI Laboratory, collaborated with
                    <a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ">Prof. Jifeng Dai</a> and <a
                        href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ">Prof. Yu Qiao</a>
                </li>
                <li>
                    2019/10 - 2020/03: Research Assistant at the University of Hong Kong (HKU), led by
                    <a href="https://scholar.google.com.hk/citations?user=aXdjxb4AAAAJ">Prof. Ping Luo</a>
                </li>
                <li>
                    2019/08 - 2020/03: Research Intern at SenseTime Group Limited, led by <a
                        href="https://scholar.google.com/citations?user=5C6zNiIAAAAJ">Xuebo Liu</a> and <a
                        href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ">Ding Liang</a>
                </li>
                <li>
                    2018/06 - 2018/12: Research Intern at Momenta, led by <a
                        href="https://scholar.google.com/citations?user=oamjJdYAAAAJ">Prof. Xiang Li</a>
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>



    <div class="section">
        <h2>Recent Works (<a href="resources/html/publication.html">[Full List]</a>)</h2>
        (* Equal contribution, † Interns, # Corresponding authors)

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_InternImage.png"
            title="Vision Transformer Adapter for Dense Predictions">
            <div><strong>InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</strong><br>
                <strong>Wenhai Wang*</strong>, Jifeng Dai*, Zhe Chen*†, Zhenhang Huang* Zhiqi Li*†, Xizhou Zhu*, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao#<br>
                CVPR, 2023 <alert>(highlight paper (2.5%))</alert><br>
                <a href="https://arxiv.org/pdf/2211.05778.pdf">[Paper]</a>
                <a href="https://github.com/OpenGVLab/InternImage">[Code]</a>
                <img src="https://img.shields.io/github/stars/OpenGVLab/InternImage?style=social"/>
                <!-- <a class="github-button" href="https://github.com/OpenGVLab/InternImage" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="resources/bibtex/arXiv_2022_InternImage.txt">[BibTex]</a>
                <br>
                <alert>A strong large-scale CNN-based fondamention model.</alert>
                <!-- <a
                    href="https://paperswithcode.com/sota/object-detection-on-coco?p=internimage-exploring-large-scale-vision"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-coco" /></a>
                <a
                    href="https://paperswithcode.com/sota/object-detection-on-coco-minival?p=internimage-exploring-large-scale-vision"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/semantic-segmentation-on-ade20k" /></a>
                <a
                    href="https://paperswithcode.com/sota/object-detection-on-lvis-v1-0-minival?p=towards-all-in-one-pre-training-via"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/towards-all-in-one-pre-training-via/object-detection-on-lvis-v1-0-minival" /></a>
                <a
                    href="https://paperswithcode.com/sota/3d-object-detection-on-nuscenes-camera-only?p=bevformer-v2-adapting-modern-image-backbones"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bevformer-v2-adapting-modern-image-backbones/3d-object-detection-on-nuscenes-camera-only" /></a> -->
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_ViT_Adapter.png"
                title="Vision Transformer Adapter for Dense Predictions">
            <div><strong>Vision Transformer Adapter for Dense Predictions</strong><br>
                Zhe Chen*†, Yuchen Duan*†, <strong>Wenhai Wang#</strong>, Junjun He, Tong Lu#, Jifeng Dai, Yu Qiao<br>
                ICLR, 2023 <alert>(spotlight paper (8.0%))</alert><br>
                <a href="https://openreview.net/pdf?id=plKu2GByCNW">[Paper]</a>
                <a href="https://github.com/czczup/ViT-Adapter">[Code]</a>
                <img src="https://img.shields.io/github/stars/czczup/ViT-Adapter?style=social"/>
                <!-- <a class="github-button" href="https://github.com/czczup/ViT-Adapter" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="resources/bibtex/arXiv_2022_ViT_Adapter.txt">[BibTex]</a>
                <br>
                <alert>We design a ViT adapter for dense prediction tasks.</alert>
                <!-- <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-ade20k?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-ade20k" /></a>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-cityscapes" /></a>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-coco-stuff-test?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-coco-stuff-test" /></a>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-pascal-context?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-pascal-context" /></a> -->
            </div>
            <div class="spanner"></div>
        </div>
    </div>


    <div class="section">
        <h2>Selected Works (<a href="resources/html/publication.html">[Full List]</a>)</h2>


        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_BEVFormer.png"
                title="BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers">
            <div><strong>BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via
                    Spatiotemporal Transformers</strong><br>
                Zhiqi Li*†, <strong>Wenhai Wang*</strong>, Hongyang Li*, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao,
                Jifeng Dai#<br>
                ECCV, 2022<br>
                <a href="https://arxiv.org/pdf/2203.17270.pdf">[Paper]</a>
                <a href="https://github.com/fundamentalvision/BEVFormer">[Code]</a>
                <img src="https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social"/>
                <!-- <a class="github-button" href="https://github.com/fundamentalvision/BEVFormer" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="resources/bibtex/arXiv_2022_BEVFormer.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/">[ECCV 2022' Top-10
                    Influential Papers]</a><br>
                <a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">[100 Most Cited AI Papers in 2022]</a>
                <br>
                <alert>A versatile camera-only framework for autonomous driving perception, e.g., 3D object
                    detection and semantic map segmentation.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper">
            <img class="paper" src="resources/paper_icon/CVMJ_2022_PVTv2.png"
                title="PVT v2: Improved Baselines with Pyramid Vision Transformer">
            <div><strong>PVT v2: Improved Baselines with Pyramid Vision Transformer</strong><br>
                <strong>Wenhai Wang#</strong>, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,
                Ping
                Luo, Ling Shao<br>
                CVMJ, 2021 <alert>(ESI highly cited paper (1%), ESI hot papers (0.1%))</alert><br>
                <a href="https://link.springer.com/article/10.1007/s41095-022-0274-8">[Paper]</a>
                <a href="https://github.com/whai362/PVT">[Code]</a>
                <img src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>
                <!-- <a class="github-button" href="https://github.com/whai362/PVT" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
                <a href="resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="resources/bibtex/CVMJ_2022_PVTv2.txt">[BibTex]</a>
                <br>
                <alert>A better PVT.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/ICCV_2021_PVT.png"
                title="Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions">
            <div><strong>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without
                    Convolutions</strong><br>
                <strong>Wenhai Wang</strong>, Enze Xie, Xiang Li, Deng-Ping Fan#, Kaitao Song, Ding Liang, Tong Lu#,
                Ping
                Luo, Ling Shao<br>
                ICCV, 2021 <alert>(oral presentation (3.4%))</alert><br>
                <a
                    href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf">[Paper]</a>
                <a href="https://github.com/whai362/PVT">[Code]</a>
                <img src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>
                <!-- <a class="github-button" href="https://github.com/whai362/PVT" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
                <a href="resources/reports/PVT_Chinese.pdf">[中译版]</a>
                <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
                <a href="resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="resources/bibtex/ICCV_2021_PVT.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/">[ICCV21' Top-10
                    Influential Papers]</a>
                <br>
                <alert>A pure Transformer backbone for dense prediction, such as object detection and semantic
                    segmentation.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <!-- <div class="paper"><img class="paper" src="resources/paper_icon/CVPR_2019_PSENet.png"
                title="Shape Robust Text Detection with Progressive Scale Expansion Network">
            <div><strong>Shape Robust Text Detection with Progressive Scale Expansion Network</strong><br>
                <strong>Wenhai Wang*</strong>, Enze Xie*, Xiang Li, Wenbo Hou, Tong Lu#, Gang Yu, Shuai Shao<br>
                CVPR, 2019<br>
                <a
                    href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">[Paper]</a>
                <a href="resources/posters/CVPR_2019_PSENet.pdf">[Poster]</a>
                <a href="https://github.com/whai362/PSENet">[Code]</a>
                <img src="https://img.shields.io/github/stars/whai362/PSENet?style=social"/> -->
                <!-- <a class="github-button" href="https://github.com/whai362/PSENet" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
                <!-- <a href="resources/bibtex/CVPR_2019_PSENet.txt">[BibTex]</a>
                <br>
                <alert>We proposed a segmentation-based text detector that can precisely detect text instances with
                    arbitrary shapes.</alert>
            </div>
            <div class="spanner"></div>
        </div> -->

        <div class="paper"><img class="paper" src="resources/paper_icon/TPAMI_2021_PolarMask++.png"
                title="PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond">
            <div><strong>PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and
                    Beyond</strong><br>
                Enze Xie*, <strong>Wenhai Wang*</strong>, Mingyu Ding, Ruimao Zhang, Ping Luo#<br>
                TPAMI, 2021<br>
                <a href="https://ieeexplore.ieee.org/document/9431650">[Paper]</a>
                <a href="https://github.com/xieenze/PolarMask">[Code]</a>
                <img src="https://img.shields.io/github/stars/xieenze/PolarMask?style=social"/>
                <!-- <a class="github-button" href="https://github.com/xieenze/PolarMask" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
                <a href="resources/bibtex/TPAMI_2021_PolarMask++.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/">[CVPR20' Top-10
                    Influential Papers]</a>
                <br>
                <alert>We extend PolarMask (CVPR'20 oral presentation (5.7%)) to several instance-level detection tasks.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/TPAMI_2021_PAN++.png"
            title="PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text">
            <div><strong>PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped
                    Text</strong><br>
                <strong>Wenhai Wang*</strong>, Enze Xie*, Xiang Li, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu#,
                Chunhua Shen<br>
                TPAMI, 2021<br>
                <a href="https://ieeexplore.ieee.org/document/9423611">[Paper]</a>
                <a href="https://github.com/whai362/PSENet">[Code1]</a>
                <img src="https://img.shields.io/github/stars/whai362/PSENet?style=social"/>
                <a href="https://github.com/whai362/pan_pp.pytorch">[Code2]</a>
                <img src="https://img.shields.io/github/stars/whai362/pan_pp.pytorch?style=social"/>
                <!-- <a class="github-button" href="https://github.com/whai362/pan_pp.pytorch" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="resources/bibtex/TPAMI_2021_PAN++.txt">[BibTex]</a>
                <br>
                <alert>We extend PSENet (CVPR'19) and PAN (ICCV'19) to a text spotting system.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <!-- <div class="paper"><img class="paper" src="resources/paper_icon/NeurIPS_2021_SegFormer.png"
                title="SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers">
            <div><strong>SegFormer: Simple and Efficient Design for Semantic Segmentation with
                    Transformers</strong><br>
                Enze Xie, <strong>Wenhai Wang</strong>, Zhiding Yu#, Anima Anandkuma, Jose M. Alvarez, Ping Luo#<br>
                NeurIPS, 2021<br>
                <a
                    href="https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf">[Paper]</a>
                <a href="https://github.com/NVlabs/SegFormer">[Code]</a>
                <img src="https://img.shields.io/github/stars/NVlabs/SegFormer?style=social"/> -->
                <!-- <a class="github-button" href="https://github.com/NVlabs/SegFormer" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
                <!-- <a href="https://zhuanlan.zhihu.com/p/379054782">[中文解读]</a>
                <a href="https://www.bilibili.com/video/BV1MV41147Ko/">[Demo]</a>
                <a href="resources/bibtex/NeurIPS_2021_SegFormer.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/">[NeurIPS21'
                    Top-10 Influential Papers]</a>
                <br>
                <alert>A simple and effective Transformer-based semantic segmentation framework.</alert>
            </div>
            <div class="spanner"></div>
        </div> -->

        <!-- <div class="paper"><img class="paper" src="resources/paper_icon/NeurIPS_2020_GFocal.png"
                title="Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection">
            <div><strong>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object
                    Detection</strong><br>
                Xiang Li, <strong>Wenhai Wang</strong>, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian
                Yang#<br>
                NeurIPS, 2020<br>
                <a
                    href="https://proceedings.neurips.cc/paper/2020/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf">[Paper]</a>
                <a href="https://github.com/implus/GFocal">[Code]</a>
                <img src="https://img.shields.io/github/stars/implus/GFocal?style=social"/> -->
                <!-- <a class="github-button" href="https://github.com/implus/GFocal" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
                <!-- <a href="resources/bibtex/NeurIPS_2020_GFocal.txt">[BibTex]</a>
                <br>
                <alert>We propose the generalized focal loss for learning the improved representations of dense
                    object
                    detector.</alert>
            </div>
            <div class="spanner"></div>
        </div> -->

        <!-- <div class="paper"><img class="paper" src="resources/paper_icon/CVPR_2019_SKNet.png"
                title="Selective Kernel Networks">
            <div><strong>Selective Kernel Networks</strong><br>
                Xiang Li, <strong>Wenhai Wang</strong>, Xiaolin Hu, Jian Yang#<br>
                CVPR, 2019<br>
                <a
                    href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">[Paper]</a>
                <a href="https://github.com/implus/SKNet">[Code]</a>
                <img src="https://img.shields.io/github/stars/implus/SKNet?style=social"/> -->
                <!-- <a class="github-button" href="https://github.com/implus/SKNet" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
                <!-- <a href="resources/bibtex/CVPR_2019_SKNet.txt">[BibTex]</a>
                <br>
                <alert> We proposed a dynamic selection mechanism in convolutional neural networks.</alert>
            </div>
            <div class="spanner"></div>
        </div> -->
    </div>




    <div class="section">
        <h2>Honors and Awards</h2>
        <div class="paper">
            <ul>
                <li>
                    2023/03: CVPR 2023 Award Candidate
                </li>
                <li>
                    2022/06: Waymo 2022 3D Camera-Only Detection Task, <a href="https://waymo.com/open/challenges/2022/3d-camera-only-detection/">1st Place (15,000 USD Bonus)</a>
                </li>
                <li>
                    2020/12: National Artificial Intelligence Challenge (NAIC) 2020, Remote Sensing Semantic
                    Segmentation Task,
                    <a href="https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm">1st Place (1,000,000 RMB Bonus)</a>
                </li>
                <li>
                    2019/12: China National Scholarship
                </li>
                <li>
                    2019/09: ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text, Task1, <a
                        href="https://rrc.cvc.uab.es/?ch=14&com=evaluation&view=method_info&task=1&m=54491">1st
                        Place</a>
                </li>
                <li>
                    2019/09: ICDAR2019 Robust Reading Challenge on Large-scale Street View Text with Partial
                    Labeling, Task1,
                    <a href="https://rrc.cvc.uab.es/?ch=16&com=evaluation&view=method_info&task=1&m=54537">2nd
                        Place</a>
                </li>
                <li>
                    2018/12: AI Challenger 2018 Autonomous Driving Perception Task, <a
                        href="https://www.leiphone.com/news/201812/DZmTgOqD8TQAHf2h.html">2nd Place (40,000 RMB
                        Bonus)</a>
                </li>
                <li>
                    2015/11: ACM-ICPC Asia Regional Contest, Silver Medal
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>




    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
            <strong>Senior Program Committee Member</strong><br>
            International Joint Conference on Artificial Intelligence (IJCAI), 2021<br>
            <br>

            <strong>Journal Reviewer</strong><br>
            IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br>
            International Journal of Computer Vision (IJCV)<br>
            IEEE Transactions on Image Processing (TIP)<br>
            IEEE Transactions on Multimedia (TMM)<br>
            Computational Visual Media Journal (CVMJ)<br>
            Pattern Recognition (PR)<br>
            <br>

            <strong>Program Committee Member/Conference Reviewer</strong><br>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, 2021, 2022, 2023<br>
            Neural Information Processing Systems (NeurIPS), 2020, 2021<br>
            International Conference on Machine Learning (ICML), 2021, 2022<br>
            International Conference on Learning Representations (ICLR), 2021<br>
            IEEE International Conference on Computer Vision (ICCV), 2021<br>
            European Conference on Computer Vision (ECCV), 2022<br>
            AAAI Conference on Artificial Intelligence (AAAI), 2022<br>
            International Joint Conference on Artificial Intelligence (IJCAI), 2022<br>
            IEEE Winter Conference on Applications of Computer Vision (WACV), 2021<br>
            Asian Conference on Computer Vision (ACCV), 2020<br>
        </div>
    </div>


    <div style='width:850px;height:300px;margin:0 auto'>
        <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
        <script type="text/javascript" id="clustrmaps"
            src="//cdn.clustrmaps.com/map_v2.js?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff&w=a"></script>
    </div>
</body>

</html>