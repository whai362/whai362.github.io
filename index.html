<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Wenhai Wang</title>
    <meta content="Wenhai Wang, https://whai362.github.io" name="keywords">
    <link rel="stylesheet" type="text/css" href="resources/css/mystyle.css">
    <link rel="stylesheet" type="text/css" href="resources/css/font.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-164510176-1');
    </script>
</head>

<body>
    <iframe class="info" src="resources/html/info.html"
        style="border: 1px solid #ddd; margin-bottom: 1em; padding: 1em; background-color: #fff;"></iframe>
    <!-- <div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;"> -->


    <div class="section">
        <h2>About Me (<a href="https://github.com/whai362">[GitHub]</a>
            <a href="https://scholar.google.com/citations?user=WM0OglcAAAAJ">[Google Scholar]</a>
            <a href="resources/cv/wangwenhai_cv.pdf">[CV]</a>)
        </h2>
        <div class="paper">
            I am a Research Scientist at <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a>, led by
            <a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ">Prof. Jifeng Dai</a> and <a
                href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ">Prof. Yu Qiao</a>
            <br><br>

            Previously, I obtained the Ph.D. degree from Department of Computer Science and Technology, Nanjing
            University (NJU) in 2021.
            My academic supervisor is <a href="https://cs.nju.edu.cn/lutong/">Prof. Tong Lu</a>.
            I received my B.E degree from Nanjing University of Science and Technology (NUST) in 2016.
            <br>

            I work very close with my friends <a href="https://scholar.google.com/citations?user=42MVVPgAAAAJ">Dr. Enze
                Xie</a> and <a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ">Prof. Xiang Li</a>.

            I was fortunate to work with <a href="https://scholar.google.com.hk/citations?user=aXdjxb4AAAAJ">Prof.
                Ping Luo</a> and <a href="https://scholar.google.com/citations?user=Ljk2BvIAAAAJ">Prof. Chunhua
                Shen</a>.

            <br><br>
            My recent works are mainly on:
            <ul>
                <li>CNN/Transformer Backbone</li>
                <li>Object Detection & Semantic/Instance/Panoptic Segmentation</li>
                <li>Vision-Language Model</li>
                <li>Autonomous Driving Perception</li>
                <li>Optical Character Recognition</li>
            </ul>
            <br>
            <alert>The fundamental vision department at <a href="https://www.shlab.org.cn/">Shanghai AI
                    Laboratory</a> is now hiring.
                If you are interested in internship/researcher positions related to computer vision, please feel
                free to contact me through the email.
            </alert>
            <!-- <br><p style='color:red'><strong>I am looking for a postdoctoral position or a full-time job. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>



    <div class="section">
        <h2 id="news">News</h2>
        <div class="paper">
            <ul>
                <li>
                    2022/11/10: We release <a href="https://arxiv.org/abs/2211.05778.pdf">InternImage</a>, setting a new record <alert>65.4 box mAP</alert> on COCO test-dev.
                </li>
                <li>
                    2022/09/14: <a href="https://arxiv.org/pdf/2206.04674.pdf">Uni-Perceiver-MoE</a> is accepted by NeurIPS 2022.
                </li>
                <li>
                    2022/07/04: Two papers are accepted by ECCV 2022. See <a href="https://arxiv.org/pdf/2203.17270.pdf">BEVFormer</a>ï¼Œ<a href="https://arxiv.org/pdf/2111.13579.pdf">VL-LTR</a>.
                </li>
                <li>
                    2022/06/20: Our team wins the
                    champion of <a href="https://waymo.com/open/challenges/2022/3d-camera-only-detection/">Waymo 2022 3D Camera-Only Detection Task (15,000 USD Bonus)</a>.
                </li>
                <li>
                    2022/05/25: An extended version of GFocalv1/v2 is accepted by TPAMI 2022.
                </li>
                <li>
                    2022/04/26: I am selected as one of <a
                        href="https://mp.weixin.qq.com/s/GfeYJ6zpGYaVHs2kKpjQcA">TechBeat 2022 Most Popular
                        Speakers</a>.
                </li>
                <li>
                    2022/03/10: <a href="https://arxiv.org/pdf/2203.17270.pdf">BEVFormer</a> ranks the 1st on <a
                        href="https://www.nuscenes.org/object-detection?externalData=all&mapData=all&modalities=Camera">nuScenes</a>
                    camera-only 3D detection benchmark.
                </li>
                <li>
                    2022/03/01: <a href="https://arxiv.org/pdf/2109.03814.pdf">Panoptic SegFormer</a> is accepted by
                    CVPR 2022.
                </li>
                <li>
                    2022/02/08: <a href="https://link.springer.com/article/10.1007/s41095-022-0274-8">PVT v2</a> is
                    accepted by CVMJ 2022.
                </li>
                <li>
                    2022/02/03: <a
                        href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf">PVT</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/">ICCV21'
                        Top-10 Influential Papers (Rank 2)</a>,
                    and <a
                        href="https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf">SegFormer</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/">NeurIPS21'
                        Top-10 Influential Papers (Rank 3)</a>.
                </li>
                <li>
                    2021/12/01: <a href="https://arxiv.org/pdf/2103.11784.pdf">URST</a> is accepted by AAAI 2022.
                </li>
                <li>
                    2021/09/30: <a
                        href="https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf">SegFormer</a>
                    is accepted by NeurIPS 2021.
                </li>
                <li>
                    2021/08/27: I defend my Ph.D. dissertation, and became a Ph.D.
                </li>
                <li>
                    2021/07/23: Two papers are accepted by ICCV 2021. See <a
                        href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf">PVT</a>
                    <a style='color:red'>(oral)</a>, <a
                        href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_DetCo_Unsupervised_Contrastive_Learning_for_Object_Detection_ICCV_2021_paper.pdf">DetCo</a>.
                </li>
                <li>
                    2021/05/05: Two papers are accepted by TPAMI 2021. See <a
                        href="https://ieeexplore.ieee.org/document/9423611">PAN++</a>, <a
                        href="https://ieeexplore.ieee.org/document/9431650">PolarMask++</a>.
                </li>
                <li>
                    2021/04/29: <a href="https://www.ijcai.org/proceedings/2021/0165.pdf">Trans2Seg</a> is accepted
                    by IJCAI 2021.
                </li>
                <li>
                    2021/04/01: We rewrite the code of PSENet, making it clear and easier to use. See <a
                        href="https://github.com/whai362/PSENet">here</a>.
                </li>
                <li>
                    2021/03/01: <a
                        href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Generalized_Focal_Loss_V2_Learning_Reliable_Localization_Quality_Estimation_for_CVPR_2021_paper.pdf">GFocalv2</a>
                    is accepted by CVPR 2021.
                </li>
                <li>
                    2021/02/24: Code of PVT is released at <a href="https://github.com/whai362/PVT">here</a>.
                </li>
                <li>
                    2021/02/11: <a
                        href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_PolarMask_Single_Shot_Instance_Segmentation_With_Polar_Representation_CVPR_2020_paper.pdf">PolarMask</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/">CVPR20' Top-10
                        Influential Papers</a>.
                </li>
                <li>
                    2020/12/21: Our team wins the champion of <a href="https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm">NAIC 2020 Remote Sensing Semantic Segmentation Task (1,000,000 RMB bonus).</a>
                </li>
                <li>
                    2020/10/13: Code of AE TextSpotter is released at <a
                        href="https://github.com/whai362/AE_TextSpotter">here</a>.
                </li>
                <li>
                    2020/09/25: <a
                        href="https://proceedings.neurips.cc/paper/2020/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf">GFocal</a>
                    is accepted by NeurIPS 2020.
                </li>
                <li>
                    2020/09/02: Code of PAN is released at <a href="https://github.com/whai362/pan_pp.pytorch">here</a>.
                </li>
                <li>
                    2020/07/03: Four papers are accepted by ECCV 2020. See <a
                        href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590443.pdf">AE
                        TextSpotter</a>,
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580681.pdf">TransLab</a>,
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520698.pdf">HGG</a>,
                    <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550647.pdf">TSRN</a>.
                </li>
                <li>
                    2020/03/10: <a
                        href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_PolarMask_Single_Shot_Instance_Segmentation_With_Polar_Representation_CVPR_2020_paper.pdf">PolarMask</a>
                    <a style='color:red'>(oral)</a> is accepted by CVPR 2020.
                </li>
                <li>
                    2019/07/23: <a
                        href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Efficient_and_Accurate_Arbitrary-Shaped_Text_Detection_With_Pixel_Aggregation_Network_ICCV_2019_paper.pdf">PAN</a>
                    is accepted by ICCV 2019.
                </li>
                <li>
                    2019/03/15: Two paper are accepted by CVPR 2019. See <a
                        href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">PSENet</a>,
                    <a
                        href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">SKNet</a>.
                </li>
                <li>
                    2018/06/16: <a href="https://www.ijcai.org/Proceedings/2018/0391.pdf">MixNet</a> <a
                        style='color:red'>(oral)</a> is accepted by IJCAI 2018.
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>



    <div class="section">
        <h2 id="experience">Experience</h2>
        <div class="paper">
            <ul>
                <li>
                    2021/09 - Present: Research Scientist at Shanghai AI Laboratory, led by
                    <a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ">Prof. Jifeng Dai</a> and <a
                        href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ">Prof. Yu Qiao</a>
                </li>
                <li>
                    2019/10 - 2020/03: Research Assistant at the University of Hong Kong (HKU), led by
                    <a href="https://scholar.google.com.hk/citations?user=aXdjxb4AAAAJ">Prof. Ping Luo</a>
                </li>
                <li>
                    2019/08 - 2020/03: Research Intern at SenseTime Group Limited, led by <a
                        href="https://scholar.google.com/citations?user=5C6zNiIAAAAJ">Xuebo Liu</a> and <a
                        href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ">Ding Liang</a>
                </li>
                <li>
                    2018/06 - 2018/12: Research Intern at Momenta, led by <a
                        href="https://scholar.google.com/citations?user=oamjJdYAAAAJ">Prof. Xiang Li</a>
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>



    <div class="section">
        <h2>Recent Works (<a href="resources/html/publication.html">[Full List]</a>)</h2>
        (* Equal contribution, â€  Interns, # Corresponding authors)

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_InternImage.png"
            title="Vision Transformer Adapter for Dense Predictions">
            <div><strong>InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</strong><br>
                <strong>Wenhai Wang*</strong>, Jifeng Dai*, Zhe Chen*â€ , Zhenhang Huang* Zhiqi Li*â€ , Xizhou Zhu*, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao#<br>
                arXiv, 2022<br>
                <a href="https://arxiv.org/pdf/2211.05778.pdf">[Paper]</a>
                <a href="https://github.com/OpenGVLab/InternImage">[Code]</a>
                <a class="github-button" href="https://github.com/OpenGVLab/InternImage" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a>
                <a href="resources/bibtex/arXiv_2022_InternImage.txt">[BibTex]</a>
                <br>
                <alert>A strong large-scale CNN-based fondamention model.</alert>
                <a
                    href="https://paperswithcode.com/sota/object-detection-on-coco?p=internimage-exploring-large-scale-vision"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-coco" /></a>
                <a
                    href="https://paperswithcode.com/sota/object-detection-on-coco-minival?p=internimage-exploring-large-scale-vision"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/semantic-segmentation-on-ade20k" /></a>
                <a
                    href="https://paperswithcode.com/sota/object-detection-on-lvis-v1-0-minival?p=towards-all-in-one-pre-training-via"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/towards-all-in-one-pre-training-via/object-detection-on-lvis-v1-0-minival" /></a>
                <a
                    href="https://paperswithcode.com/sota/3d-object-detection-on-nuscenes-camera-only?p=bevformer-v2-adapting-modern-image-backbones"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bevformer-v2-adapting-modern-image-backbones/3d-object-detection-on-nuscenes-camera-only" /></a>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_ViT_Adapter.png"
                title="Vision Transformer Adapter for Dense Predictions">
            <div><strong>Vision Transformer Adapter for Dense Predictions</strong><br>
                Zhe Chen*â€ , Yuchen Duan*â€ , <strong>Wenhai Wang*</strong>, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao#<br>
                arXiv, 2022<br>
                <a href="https://arxiv.org/pdf/2205.08534.pdf">[Paper]</a>
                <a href="https://github.com/czczup/ViT-Adapter">[Code]</a>
                <a class="github-button" href="https://github.com/czczup/ViT-Adapter" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a>
                <a href="resources/bibtex/arXiv_2022_ViT_Adapter.txt">[BibTex]</a>
                <br>
                <alert>We design a ViT adapter for dense prediction tasks.</alert>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-ade20k?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-ade20k" /></a>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-cityscapes" /></a>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-coco-stuff-test?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-coco-stuff-test" /></a>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-pascal-context?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-pascal-context" /></a>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_BEVFormer.png"
                title="BEVFormer: Learning Birdâ€™s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers">
            <div><strong>BEVFormer: Learning Birdâ€™s-Eye-View Representation from Multi-Camera Images via
                    Spatiotemporal Transformers</strong><br>
                Zhiqi Li*â€ , <strong>Wenhai Wang*</strong>, Hongyang Li*, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao,
                Jifeng Dai#<br>
                ECCV, 2022<br>
                <a href="https://arxiv.org/pdf/2203.17270.pdf">[Paper]</a>
                <a href="https://github.com/fundamentalvision/BEVFormer">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/zhiqi-li/BEVFormer?style=social"/> -->
                <a class="github-button" href="https://github.com/fundamentalvision/BEVFormer" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a>
                <a href="resources/bibtex/arXiv_2022_BEVFormer.txt">[BibTex]</a>
                <br>
                <alert>A versatile camera-only framework for autonomous driving perception, e.g., 3D object
                    detection and semantic map segmentation.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_VL_LTR.png"
                title="VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition">
            <div><strong>VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual
                    Recognition</strong><br>
                Changyao Tian*â€ , <strong>Wenhai Wang*</strong>, Xizhou Zhu*, Xiaogang Wang, Jifeng Dai#, Yu Qiao<br>
                ECCV, 2022<br>
                <a href="https://arxiv.org/pdf/2111.13579.pdf">[Paper]</a>
                <a href="https://github.com/ChangyaoTian/VL-LTR">[Code]</a>
                <a class="github-button" href="https://github.com/ChangyaoTian/VL-LTR" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a>
                <a href="resources/bibtex/arXiv_2022_VL_LTR.txt">[BibTex]</a>
                <br>
                <alert>We design a vision-language-based framework for long-tailed recognition.</alert>
                <a
                    href="https://paperswithcode.com/sota/long-tail-learning-on-imagenet-lt?p=vl-ltr-learning-class-wise-visual-linguistic"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vl-ltr-learning-class-wise-visual-linguistic/long-tail-learning-on-imagenet-lt" /></a>
                <a
                    href="https://paperswithcode.com/sota/long-tail-learning-on-inaturalist-2018?p=vl-ltr-learning-class-wise-visual-linguistic"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vl-ltr-learning-class-wise-visual-linguistic/long-tail-learning-on-inaturalist-2018" /></a>
                <a
                    href="https://paperswithcode.com/sota/long-tail-learning-on-places-lt?p=vl-ltr-learning-class-wise-visual-linguistic"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vl-ltr-learning-class-wise-visual-linguistic/long-tail-learning-on-places-lt" /></a>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/TPAMI_2022_GFocal.png"
                title="Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection">
            <div><strong>Generalized Focal Loss: Towards Efficient Representation Learning for Dense Object
                    Detection</strong><br>
                Xiang Li, Chengqi Lv, <strong>Wenhai Wang</strong>, Gang Li, Lingfeng Yang, Jian Yang#<br>
                TPAMI, 2022<br>
                <a href="https://ieeexplore.ieee.org/document/9792391">[Paper]</a>
                <a href="https://github.com/implus/GFocal">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/implus/GFocal?style=social"/> -->
                <a class="github-button" href="https://github.com/implus/GFocal" data-icon="octicon-star"
                    data-show-count="true">Star</a>
                <a href="resources/bibtex/TPAMI_2022_GFocal.txt">[BibTex]</a>
                <br>
                <alert>An extended version of GFLv1/v2.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/CVPR_2022_Panoptic_Segformer.png"
                title="Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers">
            <div><strong>Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with
                    Transformers</strong><br>
                Zhiqi Liâ€ , <strong>Wenhai Wang#</strong>, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez,
                Tong Lu#, Ping Luo<br>
                CVPR, 2022<br>
                <a href="https://arxiv.org/pdf/2109.03814.pdf">[Paper]</a>
                <a href="https://github.com/zhiqi-li/Panoptic-SegFormer">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/zhiqi-li/Panoptic-SegFormer?style=social"/> -->
                <a class="github-button" href="https://github.com/zhiqi-li/Panoptic-SegFormer" data-icon="octicon-star"
                    data-show-count="true">Star</a>
                <a href="resources/bibtex/CVPR_2022_Panoptic_Segformer.txt">[BibTex]</a>
                <br>
                <alert>We proposed a transformer-based panoptic segmentation framework.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/CVMJ_2022_PVTv2.png"
                title="PVT v2: Improved Baselines with Pyramid Vision Transformer">
            <div><strong>PVT v2: Improved Baselines with Pyramid Vision Transformer</strong><br>
                <strong>Wenhai Wang#</strong>, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,
                Ping
                Luo, Ling Shao<br>
                CVMJ, 2021<br>
                <a href="https://link.springer.com/article/10.1007/s41095-022-0274-8">[Paper]</a>
                <a href="https://github.com/whai362/PVT">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/whai362/PVT?style=social"/> -->
                <a class="github-button" href="https://github.com/whai362/PVT" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a>
                <a href="https://zhuanlan.zhihu.com/p/353222035">[ä¸­æ–‡è§£è¯»]</a>
                <a href="resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="resources/bibtex/CVMJ_2022_PVTv2.txt">[BibTex]</a>
                <br>
                <alert>A better PVT.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/AAAI_2022_URST.png"
                title="Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance Normalization">
            <div><strong>Towards Ultra-Resolution Neural Style Transfer via Thumbnail Instance
                    Normalization</strong><br>
                Zhe Chenâ€ , <strong>Wenhai Wang#</strong>, Enze Xie, Tong Lu#, Ping Luo<br>
                AAAI, 2022<br>
                <a href="https://www.aaai.org/AAAI22Papers/AAAI-8228.ZheC.pdf">[Paper]</a>
                <a href="https://github.com/czczup/URST">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/czczup/URST?style=social"/> -->
                <a class="github-button" href="https://github.com/czczup/URST" data-icon="octicon-star"
                    data-show-count="true">Star</a>
                <a href="resources/bibtex/AAAI_2022_URST.txt">[BibTex]</a>
                <br>
                <alert>We proposed a neural style transfer framework for arbitrary ultra-resolution images.</alert>
            </div>
            <div class="spanner"></div>
        </div>
    </div>


    <div class="section">
        <h2>Selected Works (<a href="resources/html/publication.html">[Full List]</a>)</h2>
        <div class="paper"><img class="paper" src="resources/paper_icon/ICCV_2021_PVT.png"
                title="Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions">
            <div><strong>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without
                    Convolutions</strong><br>
                <strong>Wenhai Wang</strong>, Enze Xie, Xiang Li, Deng-Ping Fan#, Kaitao Song, Ding Liang, Tong Lu#,
                Ping
                Luo, Ling Shao<br>
                ICCV, 2021 (<strong>oral presentation</strong>)<br>
                <a
                    href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf">[Paper]</a>
                <a href="https://github.com/whai362/PVT">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/whai362/PVT?style=social"/> -->
                <a class="github-button" href="https://github.com/whai362/PVT" data-icon="octicon-star"
                    data-show-count="true">Star</a>
                <a href="resources/reports/PVT_Chinese.pdf">[ä¸­è¯‘ç‰ˆ]</a>
                <a href="https://zhuanlan.zhihu.com/p/353222035">[ä¸­æ–‡è§£è¯»]</a>
                <a href="resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="resources/bibtex/ICCV_2021_PVT.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/">[ICCV21' Top-10
                    Influential Papers]</a>
                <br>
                <alert>A pure Transformer backbone for dense prediction, such as object detection and semantic
                    segmentation.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/TPAMI_2021_PAN++.png"
                title="PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text">
            <div><strong>PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped
                    Text</strong><br>
                <strong>Wenhai Wang*</strong>, Enze Xie*, Xiang Li, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu#,
                Chunhua Shen<br>
                TPAMI, 2021<br>
                <a href="https://ieeexplore.ieee.org/document/9423611">[Paper]</a>
                <a href="https://github.com/whai362/pan_pp.pytorch">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/whai362/pan_pp.pytorch?style=social"/> -->
                <a class="github-button" href="https://github.com/whai362/pan_pp.pytorch" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a>
                <a href="resources/bibtex/TPAMI_2021_PAN++.txt">[BibTex]</a>
                <br>
                <alert>We extend PSENet (CVPR'19) and PAN (ICCV'19) to a text spotting system.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/CVPR_2019_PSENet.png"
                title="Shape Robust Text Detection with Progressive Scale Expansion Network">
            <div><strong>Shape Robust Text Detection with Progressive Scale Expansion Network</strong><br>
                <strong>Wenhai Wang*</strong>, Enze Xie*, Xiang Li, Wenbo Hou, Tong Lu#, Gang Yu, Shuai Shao<br>
                CVPR, 2019<br>
                <a
                    href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">[Paper]</a>
                <a href="resources/posters/CVPR_2019_PSENet.pdf">[Poster]</a>
                <a href="https://github.com/whai362/PSENet">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/whai362/PSENet?style=social"/> -->
                <a class="github-button" href="https://github.com/whai362/PSENet" data-icon="octicon-star"
                    data-show-count="true">Star</a>
                <a href="resources/bibtex/CVPR_2019_PSENet.txt">[BibTex]</a>
                <br>
                <alert>We proposed a segmentation-based text detector that can precisely detect text instances with
                    arbitrary shapes.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/TPAMI_2021_PolarMask++.png"
                title="PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond">
            <div><strong>PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and
                    Beyond</strong><br>
                Enze Xie*, <strong>Wenhai Wang*</strong>, Mingyu Ding, Ruimao Zhang, Ping Luo#<br>
                TPAMI, 2021<br>
                <a href="https://ieeexplore.ieee.org/document/9431650">[Paper]</a>
                <a href="https://github.com/xieenze/PolarMask">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/xieenze/PolarMask?style=social"/> -->
                <a class="github-button" href="https://github.com/xieenze/PolarMask" data-icon="octicon-star"
                    data-show-count="true">Star</a>
                <a href="resources/bibtex/TPAMI_2021_PolarMask++.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/">[CVPR20' Top-10
                    Influential Papers]</a>
                <br>
                <alert>We extend PolarMask(CVPR'20) to several instance-level detection tasks.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/NeurIPS_2021_SegFormer.png"
                title="SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers">
            <div><strong>SegFormer: Simple and Efficient Design for Semantic Segmentation with
                    Transformers</strong><br>
                Enze Xie, <strong>Wenhai Wang</strong>, Zhiding Yu#, Anima Anandkuma, Jose M. Alvarez, Ping Luo#<br>
                NeurIPS, 2021<br>
                <a
                    href="https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf">[Paper]</a>
                <a href="https://github.com/NVlabs/SegFormer">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/NVlabs/SegFormer?style=social"/> -->
                <a class="github-button" href="https://github.com/NVlabs/SegFormer" data-icon="octicon-star"
                    data-show-count="true">Star</a>
                <a href="https://zhuanlan.zhihu.com/p/379054782">[ä¸­æ–‡è§£è¯»]</a>
                <a href="https://www.bilibili.com/video/BV1MV41147Ko/">[Demo]</a>
                <a href="resources/bibtex/NeurIPS_2021_SegFormer.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/">[NeurIPS21'
                    Top-10 Influential Papers]</a>
                <br>
                <alert>A simple and effective Transformer-based semantic segmentation framework.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/NeurIPS_2020_GFocal.png"
                title="Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection">
            <div><strong>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object
                    Detection</strong><br>
                Xiang Li, <strong>Wenhai Wang</strong>, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian
                Yang#<br>
                NeurIPS, 2020<br>
                <a
                    href="https://proceedings.neurips.cc/paper/2020/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf">[Paper]</a>
                <a href="https://github.com/implus/GFocal">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/implus/GFocal?style=social"/> -->
                <a class="github-button" href="https://github.com/implus/GFocal" data-icon="octicon-star"
                    data-show-count="true">Star</a>
                <a href="resources/bibtex/NeurIPS_2020_GFocal.txt">[BibTex]</a>
                <br>
                <alert>We propose the generalized focal loss for learning the improved representations of dense
                    object
                    detector.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/CVPR_2019_SKNet.png"
                title="Selective Kernel Networks">
            <div><strong>Selective Kernel Networks</strong><br>
                Xiang Li, <strong>Wenhai Wang</strong>, Xiaolin Hu, Jian Yang#<br>
                CVPR, 2019<br>
                <a
                    href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">[Paper]</a>
                <a href="https://github.com/implus/SKNet">[Code]</a>
                <!-- <img src="https://img.shields.io/github/stars/implus/SKNet?style=social"/> -->
                <a class="github-button" href="https://github.com/implus/SKNet" data-icon="octicon-star"
                    data-show-count="true">Star</a>
                <a href="resources/bibtex/CVPR_2019_SKNet.txt">[BibTex]</a>
                <br>
                <alert> We proposed a dynamic selection mechanism in convolutional neural networks.</alert>
            </div>
            <div class="spanner"></div>
        </div>
    </div>




    <div class="section">
        <h2>Honors and Awards</h2>
        <div class="paper">
            <ul>
                <li>
                    2022/06: Waymo 2022 3D Camera-Only Detection Task, <a href="https://waymo.com/open/challenges/2022/3d-camera-only-detection/">1st Place (15,000 USD Bonus)</a>
                </li>
                <li>
                    2020/12: National Artificial Intelligence Challenge (NAIC) 2020, Remote Sensing Semantic
                    Segmentation Task,
                    <a href="https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm">1st Place (1,000,000 RMB Bonus)</a>
                </li>
                <li>
                    2019/12: China National Scholarship
                </li>
                <li>
                    2019/09: ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text, Task1, <a
                        href="https://rrc.cvc.uab.es/?ch=14&com=evaluation&view=method_info&task=1&m=54491">1st
                        Place</a>
                </li>
                <li>
                    2019/09: ICDAR2019 Robust Reading Challenge on Large-scale Street View Text with Partial
                    Labeling, Task1,
                    <a href="https://rrc.cvc.uab.es/?ch=16&com=evaluation&view=method_info&task=1&m=54537">2nd
                        Place</a>
                </li>
                <li>
                    2018/12: AI Challenger 2018 Autonomous Driving Perception Task, <a
                        href="https://www.leiphone.com/news/201812/DZmTgOqD8TQAHf2h.html">2nd Place (40,000 RMB
                        Bonus)</a>
                </li>
                <li>
                    2015/11: ACM-ICPC Asia Regional Contest, Silver Medal
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>




    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
            <strong>Senior Program Committee Member</strong><br>
            International Joint Conference on Artificial Intelligence (IJCAI), 2021<br>
            <br>

            <strong>Journal Reviewer</strong><br>
            IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br>
            International Journal of Computer Vision (IJCV)<br>
            IEEE Transactions on Image Processing (TIP)<br>
            IEEE Transactions on Multimedia (TMM)<br>
            Computational Visual Media Journal (CVMJ)<br>
            Pattern Recognition (PR)<br>
            <br>

            <strong>Program Committee Member/Conference Reviewer</strong><br>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, 2021, 2022<br>
            Neural Information Processing Systems (NeurIPS), 2020, 2021<br>
            International Conference on Machine Learning (ICML), 2021, 2022<br>
            International Conference on Learning Representations (ICLR), 2021<br>
            IEEE International Conference on Computer Vision (ICCV), 2021<br>
            European Conference on Computer Vision (ECCV), 2022<br>
            AAAI Conference on Artificial Intelligence (AAAI), 2022<br>
            International Joint Conference on Artificial Intelligence (IJCAI), 2022<br>
            IEEE Winter Conference on Applications of Computer Vision (WACV), 2021<br>
            Asian Conference on Computer Vision (ACCV), 2020<br>
        </div>
    </div>


    <div style='width:850px;height:300px;margin:0 auto'>
        <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
        <script type="text/javascript" id="clustrmaps"
            src="//cdn.clustrmaps.com/map_v2.js?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff&w=a"></script>
    </div>
</body>

</html>