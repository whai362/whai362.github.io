<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wenhai Wang</title>
    <meta content="Wenhai Wang, https://whai362.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
        <img title="whai362" style="float: left; padding-left: .01em; height: 130px;"
             src="./resources/images/me.png">
        <div style="padding-left: 10em; vertical-align: top; height: 120px;">
            <span style="line-height: 150%; font-size: 20pt;">Wenhai Wang (王文海)</span><br>
            <span>Department of Computer Science and Technology, <a
                    href="https://www.nju.edu.cn/EN/7f/6b/c7136a163691/page.htm">Nanjing University
                        (NJU)</a></span><br>
            <span><strong>Address</strong>: 163 Xianlin Avenue, Qixia District, Nanjing, China</span><br>
            <span><strong>Email</strong>: wangwenhai362 [at] {163.com, smail.nju.edu.cn} </span> <br>
        </div>
    </div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
    <div class="section">
        <h2>About Me (<a href="https://github.com/whai362">[GitHub]</a>
            <a href="https://scholar.google.com/citations?user=WM0OglcAAAAJ">[Google Scholar]</a>
            <a href="./resources/cv/wwh_cv.pdf">[CV]</a>)
        </h2>
        <div class="paper">
            I am a Ph.D. candidate at Department of Computer Science and Technology, Nanjing University (NJU).
            My academic supervisor is <a href="https://cs.nju.edu.cn/lutong/">Prof. Tong Lu</a>.
            I received my bachelor degree from School of Computer Science and Engineering, Nanjing University of Science and Technology (NUST) in 2016.
            <br><br>

            I work very close with my friends <a href="https://scholar.google.com/citations?user=42MVVPgAAAAJ">Enze
            Xie</a> and <a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ">Xiang Li</a>.
            My recent works are mainly on scene text detection/recognition, deep
            neural networks exploration, object detection and instance segmentation.

            <br><br>
<!--            <p style='color:red'><strong>I am looking for a postdoctoral position or a full-time job. Please feel-->
<!--                free to contact me through the email.</strong></p>-->
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="news">News</h2>
        <div class="paper">
            <ul>
                <li>
                    2021-06-26: PVTv2 is released. See <a href="https://arxiv.org/pdf/2106.13797.pdf">paper</a>, <a href="https://github.com/whai362/PVT">code</a>.
                </li>
                <li>
                    2021-05-05: Two papers are accepted by TPAMI 2021. See <a href="https://ieeexplore.ieee.org/document/9423611">PAN++</a>, <a href="https://arxiv.org/pdf/2105.02184.pdf">PolarMask++</a>.
                </li>
                <li>
                    2021-04-29: <a href="https://arxiv.org/pdf/2101.08461.pdf">Trans2Seg</a> is accepted by IJCAI 2021.
                </li>
                <li>
                    2021-04-01: We rewrite the code of PSENet, making it clear and easier to use. See <a href="https://github.com/whai362/PSENet">here</a>.
                </li>
                <li>
                    2021-03-01: <a href="https://arxiv.org/pdf/2011.12885.pdf">GFocalv2</a> is accepted by CVPR 2021.
                </li>
                <li>
                    2021-02-24: Code of PVT is released at <a
                        href="https://github.com/whai362/PVT">here</a>.
                </li>
                <li>
                    2020-12-21: <a href="https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm">Our team wins the champion
                    of NAIC 2020 Remote Sensing Semantic Segmentation Task (1,000,000 RMB bonus).</a>
                </li>
                <li>
                    2020-10-13: Code of AE TextSpotter is released at <a
                        href="https://github.com/whai362/AE_TextSpotter">here</a>.
                </li>
                <li>
                    2020-09-25: <a href="https://arxiv.org/pdf/2006.04388.pdf">GFocal</a> is accepted by NeurIPS 2020.
                </li>
                <li>
                    2020-09-02: Code of PAN is released at <a
                        href="https://github.com/whai362/pan_pp.pytorch">here</a>.
                </li>
                <li>
                    2020-07-03: Four papers are accepted by ECCV 2020. See <a href="https://arxiv.org/pdf/2008.00714.pdf">AE TextSpotter</a>, 
                    <a href="https://arxiv.org/pdf/2003.13948.pdf">TransLab</a>, <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520698.pdf">HGG</a>,
                    <a href="https://arxiv.org/pdf/2005.03341.pdf">TSRN</a>.
                </li>
                <li>
                    2020-03-10: <a href="https://arxiv.org/pdf/1909.13226.pdf">PolarMask</a> is accepted by CVPR 2020.
                </li>
                <li>
                    2019-07-23: <a
                    href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Efficient_and_Accurate_Arbitrary-Shaped_Text_Detection_With_Pixel_Aggregation_Network_ICCV_2019_paper.pdf">PAN</a> is accepted by ICCV 2019.
                </li>
                <li>
                    2019-03-15: Two paper are accepted by CVPR 2019. See <a
                    href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">PSENet</a>, <a
                    href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">SKNet</a>.
                </li>
                <li>
                    2018-06-16: <a href="https://www.ijcai.org/Proceedings/2018/0391.pdf">MixNet</a> is accepted by IJCAI 2018.
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="experience">Experience</h2>
        <div class="paper">
            <ul>
                <li>
                    Oct. 2019 - Mar. 2020, research assistant at the University of Hong Kong (HKU), hosted by
                    <a href="https://scholar.google.com.hk/citations?user=aXdjxb4AAAAJ">Dr. Ping Luo</a>
                </li>
                <li>
                    Aug. 2019 - Mar. 2020, research intern at SenseTime Group Limited, hosted by <a
                        href="https://scholar.google.com/citations?user=5C6zNiIAAAAJ">Xuebo Liu</a> and <a
                        href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ">Ding Liang</a>
                </li>
                <li>
                    Jun. 2018 - Dec. 2018, research intern at Momenta, hosted by <a
                        href="https://scholar.google.com/citations?user=oamjJdYAAAAJ">Xiang Li</a>
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Selected Publications (<a href="./resources/html/publication.html">[Full List]</a>)</h2>

        <div class="paper"><img class="paper" src="./resources/paper_icon/arXiv_2021_PVTv2.png"
            title="PVTv2: Improved Baselines with Pyramid Vision Transformer">
            <div><strong>PVTv2: Improved Baselines with Pyramid Vision Transformer</strong><br>
                <strong>Wenhai Wang</strong>, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
                Luo, Ling Shao<br>
                Technical Report, 2021<br>
                <a href="https://arxiv.org/pdf/2106.13797.pdf">[Paper]</a>
                <!--                [<a href="./resources/posters/ICCV_2019_PAN.pdf">Poster</a>]-->
                <a href="https://github.com/whai362/PVT">[Code]</a><img
                    src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>
                <!-- <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a> -->
                <!-- <a href="./resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a> -->
                <!-- <a href="./resources/bibtex/arXiv_2021_PVT.txt">[BibTex]</a> -->
                <br>
                <alert>A better PVT.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/arXiv_2021_PVT.png"
                                title="Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions">
            <div><strong>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</strong><br>
                <strong>Wenhai Wang</strong>, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
                Luo, Ling Shao<br>
                Technical Report, 2021<br>
                <a href="https://arxiv.org/pdf/2102.12122.pdf">[Paper]</a>
                <!--                [<a href="./resources/posters/ICCV_2019_PAN.pdf">Poster</a>]-->
                <a href="https://github.com/whai362/PVT">[Code]</a><img
                        src="https://img.shields.io/github/stars/whai362/PVT?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
                <a href="./resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="./resources/bibtex/arXiv_2021_PVT.txt">[BibTex]</a>
                <br>
                <alert>A pure Transformer backbone for dense prediction, such as object detection and semantic segmentation.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/TPAMI_2021_PAN++.png"
            title="PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text">
            <div><strong>PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text</strong><br>
                <strong>Wenhai Wang</strong>, Enze Xie, Xiang Li, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu, Chunhua Shen<br>
                TPAMI, 2021<br>
                <a href="https://ieeexplore.ieee.org/document/9423611">[Paper]</a>
                <a href="https://github.com/whai362/pan_pp.pytorch">[Code]</a><img
                        src="https://img.shields.io/github/stars/whai362/pan_pp.pytorch?style=social"/>
                <a href="./resources/bibtex/TPAMI_2021_PAN++.txt">[BibTex]</a>
                <br>
                <alert>We extend PSENet (CVPR'19) and PAN (ICCV'19) to a text spotting system.</alert>
            </div>
            <div class="spanner"></div>
        </div>
        

        <div class="paper"><img class="paper" src="./resources/paper_icon/ECCV_2020_AE_TextSpotter.png"
                                title="AE TextSpotter: Learning Visual and Linguistic Representation for Ambiguous Text Spotting">
            <div><strong>AE TextSpotter: Learning Visual and Linguistic Representation for Ambiguous Text
                Spotting</strong><br>
                <strong>Wenhai Wang</strong>, Xuebo Liu, Xiaozhong Ji, Enze Xie, Ding Liang, Zhibo Yang, Tong Lu,
                Chunhua Shen, Ping Luo<br>
                in ECCV, 2020<br>
                <a href="https://arxiv.org/pdf/2008.00714.pdf">[Paper]</a>
                <!--                [<a href="./resources/posters/ICCV_2019_PAN.pdf">Poster</a>]-->
                <a href="https://github.com/whai362/TDA-ReCTS">[Dataset]</a>
                <a href="https://github.com/whai362/AE_TextSpotter">[Code]</a><img
                        src="https://img.shields.io/github/stars/whai362/AE_TextSpotter?style=social"/>
                <a href="./resources/bibtex/ECCV_2020_AE_TextSpotter.txt">[BibTex]</a>
                <br>
                <alert>We introduce linguistic information to eliminate the ambiguity in text detection.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ICCV_2019_PAN.png"
                                title="Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network">
            <div><strong>Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation
                Network</strong><br>
                <strong>Wenhai Wang</strong>, Enze Xie, Xiaoge Song, Yuhang Zang, Wenjia Wang, Tong Lu, Gang Yu,
                Chunhua Shen<br>
                in ICCV, 2019<br>
                <a
                        href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Efficient_and_Accurate_Arbitrary-Shaped_Text_Detection_With_Pixel_Aggregation_Network_ICCV_2019_paper.pdf">[Paper]</a>
                <a href="./resources/posters/ICCV_2019_PAN.pdf">[Poster]</a>
                <a href="https://github.com/whai362/pan_pp.pytorch">[Code]</a><img
                        src="https://img.shields.io/github/stars/whai362/pan_pp.pytorch?style=social"/>
                <a href="./resources/bibtex/ICCV_2019_PAN.txt">[BibTex]</a>
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We propose an efficient method for arbitrary-shaped text detection.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_PSENet.png"
                                title="Shape Robust Text Detection with Progressive Scale Expansion Network">
            <div><strong>Shape Robust Text Detection with Progressive Scale Expansion Network</strong><br>
                <strong>Wenhai Wang</strong>, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao<br>
                in CVPR, 2019<br>
                <a
                        href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">[Paper]</a>
                <a href="./resources/posters/CVPR_2019_PSENet.pdf">[Poster]</a>
                <a href="https://github.com/whai362/PSENet">[Code]</a><img
                        src="https://img.shields.io/github/stars/whai362/PSENet?style=social"/>
                <a href="./resources/bibtex/CVPR_2019_PSENet.txt">[BibTex]</a>
                <br>
                <alert>We proposed a segmentation-based text detector that can precisely detect text instances with
                    arbitrary shapes.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/IJCAI_2018_MixNet.png"
                                title="Mixed Link Networks">
            <div><strong>Mixed Link Networks</strong><br>
                <strong>Wenhai Wang</strong>, Xiang Li, Jian Yang, Tong Lu<br>
                in IJCAI, 2018<br>
                <a href="https://www.ijcai.org/Proceedings/2018/0391.pdf">[Paper]</a>
                <a href="./resources/posters/IJCAI_2018_MixNet.pdf">[Poster]</a>
                <a href="https://github.com/DeepInsight-PCALab/MixNet">[Code]</a><img
                        src="https://img.shields.io/github/stars/DeepInsight-PCALab/MixNet?style=social"/>
                <a href="./resources/bibtex/IJCAI_2018_MixNet.txt">[BibTex]</a>
                <br>
                <alert>We proposed an parameter-efficient convolutional neural networks for image classification.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/TPAMI_2021_PolarMask++.png"
            title="PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond">
            <div><strong>PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond</strong><br>
                Enze Xie, <strong>Wenhai Wang</strong>, Mingyu Ding, Ruimao Zhang, Ping Luo<br>
                TPAMI, 2021<br>
                <a href="https://arxiv.org/pdf/2105.02184.pdf">[Paper]</a>
                <a href="https://github.com/xieenze/PolarMask">[Code]</a><img
                        src="https://img.shields.io/github/stars/xieenze/PolarMask?style=social"/>
                <a href="./resources/bibtex/TPAMI_2021_PolarMask++.txt">[BibTex]</a>
                <br>
                <alert>We extend PolarMask(CVPR'20) to several instance-level detection tasks.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2020_PolarMask.png"
                                title="PolarMask: Single Shot Instance Segmentation with Polar Representation">
            <div><strong>PolarMask: Single Shot Instance Segmentation with Polar Representation</strong><br>
                Enze Xie, Peize Sun, Xiaoge Song, <strong>Wenhai Wang</strong>, Chunhua Shen, Ping Luo<br>
                in CVPR, 2020 (<strong>oral presentation</strong>)<br>
                <a href="https://arxiv.org/pdf/1909.13226.pdf">[Paper]</a>
                <a href="https://github.com/xieenze/PolarMask">[Code]</a><img
                        src="https://img.shields.io/github/stars/xieenze/PolarMask?style=social"/>
                <a href="https://zhuanlan.zhihu.com/p/84890413">[中文解读]</a>
                <a href="https://www.bilibili.com/video/BV1dp4y1C7Ee?from=search&amp;seid=7560478987246751367">[Talk]</a>
                <a href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/">[CVPR20' Top-10 Influential Papers]</a>
                <a href="./resources/bibtex/CVPR_2020_PolarMask.txt">[BibTex]</a>
                <br>
                <alert>We introduced a Polar Representation to reformulate the instance segmentation problem.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/NeurIPS_2020_GFocal.png"
                                title="Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection">
            <div><strong>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object
                Detection</strong><br>
                Xiang Li, <strong>Wenhai Wang</strong>, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian
                Yang<br>
                in NeurIPS, 2020<br>
                <a href="https://arxiv.org/pdf/2006.04388.pdf">[Paper]</a>
                <a href="https://github.com/implus/GFocal">[Code]</a><img
                        src="https://img.shields.io/github/stars/implus/GFocal?style=social"/>
                <a href="./resources/bibtex/NeurIPS_2020_GFocal.txt">[BibTex]</a>
                <br>
                <alert>We propose the generalized focal loss for learning the improved representations of dense object
                    detector.
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2021_GFLv2.png"
                                title="Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection">
            <div><strong>Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object
                Detection</strong><br>
                Xiang Li, <strong>Wenhai Wang</strong>, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang<br>
                in CVPR, 2021<br>
                <a href="https://arxiv.org/pdf/2011.12885.pdf">[Paper]</a>
                <a href="https://github.com/implus/GFocalV2">[Code]</a><img
                        src="https://img.shields.io/github/stars/implus/GFocalV2?style=social"/>
                <a href="./resources/bibtex/CVPR_2021_GFLv2.txt">[BibTex]</a>
                <br>
                <alert>The improved version of GFocal
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_SKNet.png"
            title="Selective Kernel Networks">
            <div><strong>Selective Kernel Networks</strong><br>
            Xiang Li, <strong>Wenhai Wang</strong>, Xiaolin Hu, Jian Yang<br>
            in CVPR, 2019<br>
            <a
                href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">[Paper]</a>
            <a href="https://github.com/implus/SKNet">[Code]</a><img
                src="https://img.shields.io/github/stars/implus/SKNet?style=social"/>
            <a href="./resources/bibtex/CVPR_2019_SKNet.txt">[BibTex]</a>
            <br>
            <alert> We proposed a dynamic selection mechanism in convolutional neural networks.</alert>
            </div>
            <div class="spanner"></div>
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Contest</h2>
        <div class="paper">
            <ul>
                <li>
                    National Artificial Intelligence Challenge (NAIC) 2020, Remote Sensing Semantic Segmentation Task,
                    <a href="https://mp.weixin.qq.com/s/iktrS9If12uum0_qQ1MZ1A">1st Place (1,000,000 RMB Bonus)</a>.
                </li>
                <li>
                    ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text, Task1, <a
                        href="https://rrc.cvc.uab.es/?ch=14&com=evaluation&view=method_info&task=1&m=54491">1st
                    Place</a>.
                </li>
                <li>
                    ICDAR2019 Robust Reading Challenge on Large-scale Street View Text with Partial Labeling, Task1,
                    <a href="https://rrc.cvc.uab.es/?ch=16&com=evaluation&view=method_info&task=1&m=54537">2nd
                        Place</a>.
                </li>
                <li>
                    AI Challenger 2018 Autonomous Driving Perception Task, <a
                        href="https://www.leiphone.com/news/201812/DZmTgOqD8TQAHf2h.html">2nd Place (40,000 RMB
                    Bonus)</a>
                </li>
                <li>
                    ACM-ICPC Asia Regional Contest, Silver Medal
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
            <strong>Journal Reviewer</strong><br>
            IEEE Transactions on Image Processing (TIP)<br>
            IEEE Transactions on Multimedia (TMM)<br>
            Computational Visual Media Journal<br>
            <br>
            
            <strong>(Senior) Program Committee Member/Conference Reviewer</strong><br>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, 2021<br>
            Neural Information Processing Systems (NeurIPS), 2020, 2021<br>
            International Conference on Machine Learning (ICML), 2021<br>
            International Conference on Learning Representations (ICLR), 2021<br>
            IEEE International Conference on Computer Vision (ICCV), 2021<br>
            International Joint Conference on Artificial Intelligence (IJCAI), 2021<br>
            IEEE Winter Conference on Applications of Computer Vision (WACV), 2021<br>
            Asian Conference on Computer Vision (ACCV), 2020<br>
        </div>
    </div>
</div>

<div style='width:600px;height:300px;margin:0 auto'>
    <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
    <script type="text/javascript" id="clustrmaps"
            src="//cdn.clustrmaps.com/map_v2.js?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff&w=a"></script>
</div>

</body>

</html>