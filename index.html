<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Wenhai Wang</title>
    <meta content="Wenhai Wang, https://whai362.github.io" name="keywords">
    <link rel="stylesheet" type="text/css" href="resources/css/mystyle.css">
    <link rel="stylesheet" type="text/css" href="resources/css/font.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-164510176-1');
    </script>
</head>

<body>
    <!-- <iframe class="info" scrolling="no"  src="resources/html/info.html"
        style="border: 1px solid #ddd; margin-bottom: 1em; padding: 1em; background-color: #fff;"></iframe> -->
    <div class="section">
        <div class="paper">
            <img class="info" title="whai362" style="float: left; padding-left: .01em;" src="resources/images/me.png">
            <!-- <img class="info" title="affiliation" style="float: right; padding-right: .01em;" src="resources/images/affiliation1.png"> -->
            <div class="info" style="padding-left: 11.5em; vertical-align: top;">
                <span style="line-height: 150%; font-size: 25pt;">Wenhai Wang (王文海)</span><br>
                <!-- <strong>Affiliation</strong>: OpenGVLab, <a href="https://www.shlab.org.cn/">Shanghai AI
                        Laboratory</a><br>
                <strong>Address</strong>: 701 Yunjin Road, Xuhui District, Shanghai, China<br> -->
                <strong>Affiliation</strong>: <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>, The Chinese University
                of Hong Kong<br>
                <strong>Address</strong>: Room 703, Ho Sin Hang Engineering Building, The Chinese University of Hong
                Kong, Shatin, N.T. Hong Kong<br>
                <strong>Email</strong>: wangwenhai362[at]{163.com, gmail.com}, whwang@ie.cuhk.edu.hk<br>
            </div>
            <div class="spanner"></div>
        </div>
    </div>

    <div class="section">
        <h2>About Me (<a href="https://github.com/whai362">[GitHub]</a>
            <a href="https://scholar.google.com/citations?user=WM0OglcAAAAJ">[Google Scholar]</a>)
            <!-- <a href="resources/cv/wangwenhai_cv.pdf">[CV]</a>) -->
        </h2>
        <div class="paper">
            I am currently a Postdoctoral Researcher at <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>, The Chinese
            University of Hong Kong, and also collaborated with
            <a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ">Prof. Jifeng Dai</a> and <a
                href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ">Prof. Yu Qiao</a> at <a
                href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a>.
            <br><br>

            Previously, I obtained the Ph.D. degree from Department of Computer Science and Technology, Nanjing
            University (NJU) in 2021.
            My academic supervisor is <a href="https://cs.nju.edu.cn/lutong/">Prof. Tong Lu</a>.
            I received my B.E degree from Nanjing University of Science and Technology (NUST) in 2016.
            I work very close with my friends <a href="https://scholar.google.com/citations?user=42MVVPgAAAAJ">Dr. Enze
                Xie</a> and <a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ">Prof. Xiang Li</a>.


            I was fortunate to work with <a href="https://scholar.google.com.hk/citations?user=aXdjxb4AAAAJ">Prof.
                Ping Luo</a> and <a href="https://scholar.google.com/citations?user=Ljk2BvIAAAAJ">Prof. Chunhua
                Shen</a>.

            <br><br>
            My recent works are mainly on:
            <ul>
                <li>Large-Scale Foundation Models</li>
                <li>Object Detection & Segmentation</li>
                <li>Autonomous Driving Perception</li>
                <li>Optical Character Recognition</li>
            </ul>
            <!-- <br>
            <alert>OpenGVLab at <a href="https://www.shlab.org.cn/">Shanghai AI
                    Laboratory</a> is now hiring.
                If you are interested in internship/researcher positions related to computer vision, please feel
                free to contact me through the email.
            </alert> -->
            <!-- <br><p style='color:red'><strong>I am looking for a postdoctoral position or a full-time job. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>



    <div class="section">
        <h2 id="news">News</h2>
        <div class="paper">
            <ul>
                <li>
                    2023/08: <a
                        href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf">PVT</a>
                    wins <a href="https://sh.cctv.com/2023/08/25/ARTI1qS2PGWQ9TKB7HuhdDZB230825.shtml">the 2023 World
                        Artificial Intelligence Conference Youth Outstanding Paper Award</a>.
                </li>
                <li>
                    2023/08: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792391">GFL</a> is selected
                    as one of ESI Highly Cited Papers (1%).
                </li>
                <li>
                    2023/06: <a href="https://arxiv.org/abs/2212.10156">UniAD</a> wins <a
                        href="https://cvpr2023.thecvf.com/Conferences/2023/Awards">the Best Paper Award of CVPR
                        2023</a>.
                </li>
                <li>
                    2023/05: <a href="https://arxiv.org/pdf/2203.17270.pdf">BEVFormer</a> is mentioned in <a
                        href="https://www.youtube.com/watch?v=i-wpzS9ZsCs&ab_channel=NVIDIA">NVIDIA Keynote at COMPUTEX
                        2023 (1:31:48)</a>.
                </li>
                <li>
                    2023/04: <a href="https://link.springer.com/article/10.1007/s41095-022-0274-8">PVT v2</a> wins <a
                        href="https://www.springer.com/journal/41095/updates/25223320">CVMJ 2022 Honorable Mention
                        Award</a>.
                </li>
                <li>
                    2023/04: <a
                        href="https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf">SegFormer</a>
                    is included in the <a
                        href="https://developer.nvidia.com/blog/ai-models-recap-scalable-pretrained-models-across-industries/">NVIDIA
                        AI Models Recap 2022</a>, and is commented as visionary research for world-class image control.
                </li>
                <li>
                    2023/01: <a href="https://arxiv.org/pdf/2203.17270.pdf">BEVFormer</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/">ECCV 2022
                        Top-10
                        Influential Papers</a> and <a
                        href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">the 100
                        most cited AI papers in 2022</a>.
                </li>
                <li>
                    2023/01: <a href="https://link.springer.com/article/10.1007/s41095-022-0274-8">PVT v2</a> is
                    selected as one of <a href="https://mp.weixin.qq.com/s/xdrKpK2dCShWUWmISYM6Yw">ESI Highly Cited
                        Paper (1%) and ESI Hot Papers (0.1%)</a>.
                </li>
                <li>
                    2022/11: We release <a href="https://arxiv.org/abs/2211.05778.pdf">InternImage</a>, setting a new
                    record <alert>65.4 box mAP</alert> on COCO test-dev.
                </li>
                <li>
                    2022/06: Our team wins the
                    champion of <a href="https://waymo.com/open/challenges/2022/3d-camera-only-detection/">Waymo 2022 3D
                        Camera-Only Detection Task (15,000 USD Bonus)</a>.
                </li>
                <li>
                    2022/06: I wins the <a href="https://cs.nju.edu.cn/be/69/c1654a573033/page.htm">Outstanding Doctoral
                        Thesis Award</a> of Nanjing University.
                </li>
                <li>
                    2022/04: I am selected as one of <a
                        href="https://mp.weixin.qq.com/s/GfeYJ6zpGYaVHs2kKpjQcA">TechBeat 2022 Most Popular
                        Speakers</a>.
                </li>
                <li>
                    2022/02: <a
                        href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf">PVT</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/">ICCV 2021
                        Top-10 Influential Papers (Rank 2)</a>,
                    and <a
                        href="https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf">SegFormer</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/">NeurIPS 2021
                        Top-10 Influential Papers (Rank 3)</a>.
                </li>
                <li>
                    2021/02: <a
                        href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_PolarMask_Single_Shot_Instance_Segmentation_With_Polar_Representation_CVPR_2020_paper.pdf">PolarMask</a>
                    is selected as one of <a
                        href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/">CVPR 2020 Top-10
                        Influential Papers</a>.
                </li>
                <li>
                    2020/12: Our team wins the champion of <a
                        href="https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm">NAIC 2020 Remote Sensing Semantic
                        Segmentation Task (1,000,000 RMB bonus).</a>
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>



    <div class="section">
        <h2 id="experience">Experience</h2>
        <div class="paper">
            <ul>
                <li>
                    2023/04 - Present: Postdoctoral Researcher at MMLab, The Chinese University of Hong Kong, led by <a
                        href="https://scholar.google.com/citations?user=qpBtpGsAAAAJ">Prof. Xiaoou Tang</a>
                </li>
                <li>
                    2021/09 - 2023/04: Research Scientist at Shanghai AI Laboratory, collaborated with
                    <a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ">Prof. Jifeng Dai</a> and <a
                        href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ">Prof. Yu Qiao</a>
                </li>
                <li>
                    2019/10 - 2020/03: Research Assistant at HKU-MMLab, The University of Hong Kong (HKU), led by
                    <a href="https://scholar.google.com.hk/citations?user=aXdjxb4AAAAJ">Prof. Ping Luo</a>
                </li>
                <li>
                    2019/08 - 2020/03: Research Intern at SenseTime Group Limited, led by <a
                        href="https://scholar.google.com/citations?user=5C6zNiIAAAAJ">Xuebo Liu</a> and <a
                        href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ">Ding Liang</a>
                </li>
                <li>
                    2018/06 - 2018/12: Research Intern at Momenta, led by <a
                        href="https://scholar.google.com/citations?user=oamjJdYAAAAJ">Prof. Xiang Li</a>
                </li>
            </ul>

            <div class="spanner"></div>
        </div>
    </div>



    <div class="section">
        <h2>Recent Works (<a href="resources/html/publication.html">[Full List]</a>)</h2>
        (* Equal contribution, † Interns, # Corresponding authors)

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2023_AS.png"
                title="The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World">
            <div><strong>The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open
                    World</strong><br>
                Weiyun Wang*, Min Shi*, Qingyun Li*, <strong>Wenhai Wang*</strong>, Zhenhang Huang*, Linjie Xing*, Zhe
                Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, Yushi Chen, Tong Lu, Jifeng Dai#, Yu Qiao<br>
                Technical Report, 2023<br>
                <a href="https://arxiv.org/pdf/2308.01907.pdf">[Paper]</a>
                <a href="https://github.com/OpenGVLab/All-Seeing">[Code]</a>
                <a href="https://huggingface.co/spaces/OpenGVLab/all-seeing">[Demo of AS-1B]</a>
                <a href="https://openxlab.org.cn/apps/detail/wangweiyun/All-Seeing-Model-Demo">[Demo of ASM]</a>
                <img src="https://img.shields.io/github/stars/OpenGVLab/all-seeing?style=social" />
                <a href="resources/bibtex/arXiv_2023_AS.txt">[BibTex]</a>
                <br>
                <alert>Now you can see all thing in open world.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2023_VisionLLM.png"
                title="VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks">
            <div><strong>VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks</strong><br>
                <strong>Wenhai Wang*</strong>, Zhe Chen*, Xiaokang Chen*, Jiannan Wu*, Xizhou Zhu, Gang Zeng, Ping Luo,
                Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai#<br>
                Technical Report, 2023<br>
                <a href="https://arxiv.org/pdf/2305.11175.pdf">[Paper]</a>
                <a href="https://github.com/OpenGVLab/VisionLLM">[Code]</a>
                <!-- <a href="https://igpt.opengvlab.com/">[Demo]</a> -->
                <img src="https://img.shields.io/github/stars/OpenGVLab/VisionLLM?style=social" />
                <a href="resources/bibtex/arXiv_2023_VisionLLM.txt">[BibTex]</a>
                <br>
                <alert>You can custom</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2023_iGPT.png"
                title="InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language">
            <div><strong>InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond
                    Language</strong><br>
                Zhaoyang Liu*, Yinan He*, <strong>Wenhai Wang*</strong>, Weiyun Wang*, Yi Wang*, Shoufa Chen*, Qinglong
                Zhang*, Yang Yang*, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang,
                Limin Wang, Ping Luo, Jifeng Dai, Yu Qiao<br>
                Technical Report, 2023<br>
                <a href="https://arxiv.org/pdf/2305.05662.pdf">[Paper]</a>
                <a href="https://github.com/OpenGVLab/InternGPT">[Code]</a>
                <a href="https://igpt.opengvlab.com/">[Demo]</a>
                <img src="https://img.shields.io/github/stars/OpenGVLab/InternGPT?style=social" />
                <a href="resources/bibtex/arXiv_2023_iGPT.txt">[BibTex]</a>
                <br>
                <alert>InternGPT allows you to interact with ChatGPT by clicking, dragging and drawing using a pointing
                    device.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_InternImage.png"
                title="InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions">
            <div><strong>InternImage: Exploring Large-Scale Vision Foundation Models with Deformable
                    Convolutions</strong><br>
                <strong>Wenhai Wang*</strong>, Jifeng Dai*, Zhe Chen*†, Zhenhang Huang* Zhiqi Li*†, Xizhou Zhu*, Xiaowei
                Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao#<br>
                CVPR, 2023 <alert>(Highlight Paper (2.5%))</alert><br>
                <a href="https://arxiv.org/pdf/2211.05778.pdf">[Paper]</a>
                <a href="https://github.com/OpenGVLab/InternImage">[Code]</a>
                <img src="https://img.shields.io/github/stars/OpenGVLab/InternImage?style=social" />
                <!-- <a class="github-button" href="https://github.com/OpenGVLab/InternImage" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="resources/bibtex/arXiv_2022_InternImage.txt">[BibTex]</a>
                <br>
                <alert>A strong large-scale CNN-based fondamention model.</alert>
                <!-- <a
                    href="https://paperswithcode.com/sota/object-detection-on-coco?p=internimage-exploring-large-scale-vision"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-coco" /></a>
                <a
                    href="https://paperswithcode.com/sota/object-detection-on-coco-minival?p=internimage-exploring-large-scale-vision"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/semantic-segmentation-on-ade20k" /></a>
                <a
                    href="https://paperswithcode.com/sota/object-detection-on-lvis-v1-0-minival?p=towards-all-in-one-pre-training-via"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/towards-all-in-one-pre-training-via/object-detection-on-lvis-v1-0-minival" /></a>
                <a
                    href="https://paperswithcode.com/sota/3d-object-detection-on-nuscenes-camera-only?p=bevformer-v2-adapting-modern-image-backbones"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bevformer-v2-adapting-modern-image-backbones/3d-object-detection-on-nuscenes-camera-only" /></a> -->
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_ViT_Adapter.png"
                title="Vision Transformer Adapter for Dense Predictions">
            <div><strong>Vision Transformer Adapter for Dense Predictions</strong><br>
                Zhe Chen*†, Yuchen Duan*†, <strong>Wenhai Wang#</strong>, Junjun He, Tong Lu#, Jifeng Dai, Yu Qiao<br>
                ICLR, 2023 <alert>(Spotlight Paper (8.0%))</alert><br>
                <a href="https://openreview.net/pdf?id=plKu2GByCNW">[Paper]</a>
                <a href="https://github.com/czczup/ViT-Adapter">[Code]</a>
                <img src="https://img.shields.io/github/stars/czczup/ViT-Adapter?style=social" />
                <!-- <a class="github-button" href="https://github.com/czczup/ViT-Adapter" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="resources/bibtex/arXiv_2022_ViT_Adapter.txt">[BibTex]</a>
                <br>
                <alert>We design a ViT adapter for dense prediction tasks.</alert>
                <!-- <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-ade20k?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-ade20k" /></a>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-cityscapes" /></a>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-coco-stuff-test?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-coco-stuff-test" /></a>
                <a
                    href="https://paperswithcode.com/sota/semantic-segmentation-on-pascal-context?p=vision-transformer-adapter-for-dense"><img
                        style="margin-top: 3px"
                        src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/vision-transformer-adapter-for-dense/semantic-segmentation-on-pascal-context" /></a> -->
            </div>
            <div class="spanner"></div>
        </div>
    </div>


    <div class="section">
        <h2>Selected Works (<a href="resources/html/publication.html">[Full List]</a>)</h2>


        <div class="paper"><img class="paper" src="resources/paper_icon/arXiv_2022_BEVFormer.png"
                title="BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers">
            <div><strong>BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via
                    Spatiotemporal Transformers</strong><br>
                Zhiqi Li*†, <strong>Wenhai Wang*</strong>, Hongyang Li*, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao,
                Jifeng Dai#<br>
                ECCV, 2022<br>
                <a href="https://arxiv.org/pdf/2203.17270.pdf">[Paper]</a>
                <a href="https://github.com/fundamentalvision/BEVFormer">[Code]</a>
                <img src="https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social" />
                <!-- <a class="github-button" href="https://github.com/fundamentalvision/BEVFormer" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="resources/bibtex/arXiv_2022_BEVFormer.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2023/01/most-influential-eccv-papers-2023-01/">[ECCV 2022' Top-10
                    Influential Papers]</a><br>
                <a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">[100 Most Cited
                    AI Papers in 2022]</a>
                <br>
                <alert>A versatile camera-only framework for autonomous driving perception, e.g., 3D object
                    detection and semantic map segmentation.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper">
            <img class="paper" src="resources/paper_icon/CVMJ_2022_PVTv2.png"
                title="PVT v2: Improved Baselines with Pyramid Vision Transformer">
            <div><strong>PVT v2: Improved Baselines with Pyramid Vision Transformer</strong><br>
                <strong>Wenhai Wang#</strong>, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,
                Ping
                Luo, Ling Shao<br>
                CVMJ, 2021 <alert>(ESI Highly Cited Paper (1%))</alert><br>
                <a href="https://link.springer.com/article/10.1007/s41095-022-0274-8">[Paper]</a>
                <a href="https://github.com/whai362/PVT">[Code]</a>
                <img src="https://img.shields.io/github/stars/whai362/PVT?style=social" />
                <!-- <a class="github-button" href="https://github.com/whai362/PVT" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
                <a href="resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="resources/bibtex/CVMJ_2022_PVTv2.txt">[BibTex]</a><br>
                <a href="https://www.springer.com/journal/41095/updates/25223320">[CVMJ 2022 Honorable Mention
                    Award]</a>
                <br>
                <alert>A better PVT.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/ICCV_2021_PVT.png"
                title="Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions">
            <div><strong>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without
                    Convolutions</strong><br>
                <strong>Wenhai Wang</strong>, Enze Xie, Xiang Li, Deng-Ping Fan#, Kaitao Song, Ding Liang, Tong Lu#,
                Ping
                Luo, Ling Shao<br>
                ICCV, 2021 <alert>(Oral Presentation (3.4%))</alert><br>
                <a
                    href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.pdf">[Paper]</a>
                <a href="https://github.com/whai362/PVT">[Code]</a>
                <img src="https://img.shields.io/github/stars/whai362/PVT?style=social" />
                <!-- <a class="github-button" href="https://github.com/whai362/PVT" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
                <a href="resources/reports/PVT_Chinese.pdf">[中译版]</a>
                <a href="https://zhuanlan.zhihu.com/p/353222035">[中文解读]</a>
                <a href="resources/reports/wangwenhai_vision_transformer.pdf">[Report]</a>
                <a href="https://www.techbeat.net/talk-info?id=562">[Talk]</a>
                <a href="resources/bibtex/ICCV_2021_PVT.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2022/02/most-influential-iccv-papers-2022-02/">[ICCV21' Top-10
                    Influential Papers]</a>
                <br>
                <alert>A pure Transformer backbone for dense prediction, such as object detection and semantic
                    segmentation.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <!-- <div class="paper"><img class="paper" src="resources/paper_icon/CVPR_2019_PSENet.png"
                title="Shape Robust Text Detection with Progressive Scale Expansion Network">
            <div><strong>Shape Robust Text Detection with Progressive Scale Expansion Network</strong><br>
                <strong>Wenhai Wang*</strong>, Enze Xie*, Xiang Li, Wenbo Hou, Tong Lu#, Gang Yu, Shuai Shao<br>
                CVPR, 2019<br>
                <a
                    href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">[Paper]</a>
                <a href="resources/posters/CVPR_2019_PSENet.pdf">[Poster]</a>
                <a href="https://github.com/whai362/PSENet">[Code]</a>
                <img src="https://img.shields.io/github/stars/whai362/PSENet?style=social"/> -->
        <!-- <a class="github-button" href="https://github.com/whai362/PSENet" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
        <!-- <a href="resources/bibtex/CVPR_2019_PSENet.txt">[BibTex]</a>
                <br>
                <alert>We proposed a segmentation-based text detector that can precisely detect text instances with
                    arbitrary shapes.</alert>
            </div>
            <div class="spanner"></div>
        </div> -->

        <div class="paper"><img class="paper" src="resources/paper_icon/TPAMI_2021_PolarMask++.png"
                title="PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond">
            <div><strong>PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and
                    Beyond</strong><br>
                Enze Xie*, <strong>Wenhai Wang*</strong>, Mingyu Ding, Ruimao Zhang, Ping Luo#<br>
                TPAMI, 2021<br>
                <a href="https://ieeexplore.ieee.org/document/9431650">[Paper]</a>
                <a href="https://github.com/xieenze/PolarMask">[Code]</a>
                <img src="https://img.shields.io/github/stars/xieenze/PolarMask?style=social" />
                <!-- <a class="github-button" href="https://github.com/xieenze/PolarMask" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
                <a href="resources/bibtex/TPAMI_2021_PolarMask++.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/">[CVPR 2020 Top-10
                    Influential Papers]</a>
                <br>
                <alert>We extend PolarMask (CVPR 2020 Oral Presentation (5.7%)) to several instance-level detection
                    tasks.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="resources/paper_icon/TPAMI_2021_PAN++.png"
                title="PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text">
            <div><strong>PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped
                    Text</strong><br>
                <strong>Wenhai Wang*</strong>, Enze Xie*, Xiang Li, Xuebo Liu, Ding Liang, Zhibo Yang, Tong Lu#,
                Chunhua Shen<br>
                TPAMI, 2021<br>
                <a href="https://ieeexplore.ieee.org/document/9423611">[Paper]</a>
                <a href="https://github.com/whai362/PSENet">[Code1]</a>
                <img src="https://img.shields.io/github/stars/whai362/PSENet?style=social" />
                <a href="https://github.com/whai362/pan_pp.pytorch">[Code2]</a>
                <img src="https://img.shields.io/github/stars/whai362/pan_pp.pytorch?style=social" />
                <!-- <a class="github-button" href="https://github.com/whai362/pan_pp.pytorch" data-icon="octicon-star"
                    data-show-count="true" aria-label="Star ntkme/github-buttons on GitHub">Star</a> -->
                <a href="resources/bibtex/TPAMI_2021_PAN++.txt">[BibTex]</a>
                <br>
                <alert>We extend PSENet (CVPR 2019) and PAN (ICCV 2019) to a text spotting system.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <!-- <div class="paper"><img class="paper" src="resources/paper_icon/NeurIPS_2021_SegFormer.png"
                title="SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers">
            <div><strong>SegFormer: Simple and Efficient Design for Semantic Segmentation with
                    Transformers</strong><br>
                Enze Xie, <strong>Wenhai Wang</strong>, Zhiding Yu#, Anima Anandkuma, Jose M. Alvarez, Ping Luo#<br>
                NeurIPS, 2021<br>
                <a
                    href="https://proceedings.neurips.cc/paper/2021/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf">[Paper]</a>
                <a href="https://github.com/NVlabs/SegFormer">[Code]</a>
                <img src="https://img.shields.io/github/stars/NVlabs/SegFormer?style=social"/> -->
        <!-- <a class="github-button" href="https://github.com/NVlabs/SegFormer" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
        <!-- <a href="https://zhuanlan.zhihu.com/p/379054782">[中文解读]</a>
                <a href="https://www.bilibili.com/video/BV1MV41147Ko/">[Demo]</a>
                <a href="resources/bibtex/NeurIPS_2021_SegFormer.txt">[BibTex]</a><br>
                <a href="https://www.paperdigest.org/2022/02/most-influential-nips-papers-2022-02/">[NeurIPS21'
                    Top-10 Influential Papers]</a>
                <br>
                <alert>A simple and effective Transformer-based semantic segmentation framework.</alert>
            </div>
            <div class="spanner"></div>
        </div> -->

        <!-- <div class="paper"><img class="paper" src="resources/paper_icon/NeurIPS_2020_GFocal.png"
                title="Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection">
            <div><strong>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object
                    Detection</strong><br>
                Xiang Li, <strong>Wenhai Wang</strong>, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian
                Yang#<br>
                NeurIPS, 2020<br>
                <a
                    href="https://proceedings.neurips.cc/paper/2020/file/f0bda020d2470f2e74990a07a607ebd9-Paper.pdf">[Paper]</a>
                <a href="https://github.com/implus/GFocal">[Code]</a>
                <img src="https://img.shields.io/github/stars/implus/GFocal?style=social"/> -->
        <!-- <a class="github-button" href="https://github.com/implus/GFocal" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
        <!-- <a href="resources/bibtex/NeurIPS_2020_GFocal.txt">[BibTex]</a>
                <br>
                <alert>We propose the generalized focal loss for learning the improved representations of dense
                    object
                    detector.</alert>
            </div>
            <div class="spanner"></div>
        </div> -->

        <!-- <div class="paper"><img class="paper" src="resources/paper_icon/CVPR_2019_SKNet.png"
                title="Selective Kernel Networks">
            <div><strong>Selective Kernel Networks</strong><br>
                Xiang Li, <strong>Wenhai Wang</strong>, Xiaolin Hu, Jian Yang#<br>
                CVPR, 2019<br>
                <a
                    href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">[Paper]</a>
                <a href="https://github.com/implus/SKNet">[Code]</a>
                <img src="https://img.shields.io/github/stars/implus/SKNet?style=social"/> -->
        <!-- <a class="github-button" href="https://github.com/implus/SKNet" data-icon="octicon-star"
                    data-show-count="true">Star</a> -->
        <!-- <a href="resources/bibtex/CVPR_2019_SKNet.txt">[BibTex]</a>
                <br>
                <alert> We proposed a dynamic selection mechanism in convolutional neural networks.</alert>
            </div>
            <div class="spanner"></div>
        </div> -->
    </div>




    <div class="section">
        <h2>Honors and Awards</h2>
        <div class="paper">
            <ul>
                <li>
                    2023/08: <a href="https://sh.cctv.com/2023/08/25/ARTI1qS2PGWQ9TKB7HuhdDZB230825.shtml">2023 World
                        Artificial Intelligence Conference Youth Outstanding Paper Award</a>
                </li>
                <li>
                    2023/06: <a href="https://cvpr2023.thecvf.com/Conferences/2023/Awards">CVPR 2023 Best Paper
                        Award</a>
                </li>
                <li>
                    2023/04: <a href="https://www.springer.com/journal/41095/updates/25223320">CVMJ 2022 Honorable
                        Mention Award</a>
                </li>
                <li>
                    2022/06: Waymo 2022 3D Camera-Only Detection Task, <a
                        href="https://waymo.com/open/challenges/2022/3d-camera-only-detection/">1st Place (15,000 USD
                        Bonus)</a>
                </li>
                <li>
                    2022/06: <a href="https://cs.nju.edu.cn/be/69/c1654a573033/page.htm">Outstanding Doctoral Thesis
                        Award</a>, Nanjing University
                </li>
                <li>
                    2022/04: <a href="https://mp.weixin.qq.com/s/GfeYJ6zpGYaVHs2kKpjQcA">Most Popular Speakers in
                        TechBeat 2022</a>
                </li>
                <li>
                    2021/04: Outstanding Graduate, Nanjing University
                </li>
                <li>
                    2020/12: National Artificial Intelligence Challenge (NAIC) 2020, Remote Sensing Semantic
                    Segmentation Task,
                    <a href="https://cs.nju.edu.cn/b8/d5/c1654a506069/page.htm">1st Place (1,000,000 RMB Bonus)</a>
                </li>
                <li>
                    2019/12: China National Scholarship
                </li>
                <li>
                    2019/09: ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text, Task1, <a
                        href="https://rrc.cvc.uab.es/?ch=14&com=evaluation&view=method_info&task=1&m=54491">1st
                        Place</a>
                </li>
                <li>
                    2019/09: ICDAR2019 Robust Reading Challenge on Large-scale Street View Text with Partial
                    Labeling, Task1,
                    <a href="https://rrc.cvc.uab.es/?ch=16&com=evaluation&view=method_info&task=1&m=54537">2nd
                        Place</a>
                </li>
                <li>
                    2018/12: AI Challenger 2018 Autonomous Driving Perception Task, <a
                        href="https://www.leiphone.com/news/201812/DZmTgOqD8TQAHf2h.html">2nd Place (40,000 RMB
                        Bonus)</a>
                </li>
                <li>
                    2015/11: ACM-ICPC Asia Regional Contest, Silver Medal
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>

    <div class="section">
        <h2>Invited Talk</h2>
        <div class="paper">
            <ul>
                <li>2023/08: Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions, <a href="https://sh.cctv.com/2023/08/25/ARTI1qS2PGWQ9TKB7HuhdDZB230825.shtml">WAIC Youth Outstanding Paper Award Talk</a>
                </li>
                <li>
                    2023/07-08: Preliminary Study on "Large-Scale Visual Basic Model + LLM", <a href="https://apposcmf8kb5033.pc.xiaoe-tech.com/detail/l_64e46f0de4b0b0bc2c11c0f3/4?fromH5=true">Zhidx</a>/<a href="https://www.chaspark.com/#/coffeeHours/activities/904504974487224320">Huawei Noah's Ark Lab</a>/Tencent Youtu Lab/Fudan University Talk
                </li>
                <li>
                    2023/06: Study and Application of Large-scale Foundation Models in Open World Tasks, <a href="http://valser.org/2023/#/workshopde?id=19">VALSE Talk</a>
                </li>
                <li>
                    2023/05: InternImage: A Large-Scale Generic Vision Model, SenseTime Talk
                </li>
                <li>
                    2022/11-12: Study and Application of Multi-Task Generic Perception Model, <a
                        href="https://www.bilibili.com/video/av349382729/?vd_source=94a5b418d6cf1cb0e66ece685c243f14">AITIME (2:31:30)</a>/Tsinghua University Talk
                </li>
                <li>
                    2022/07: Transformer-based Vision Perception, <a href="http://chinamm.csig.org.cn/2022/forum.html">ChinaMM Talk</a>
                </li>
                <li>
                    2021/07: Application of Transformer in Detection and Segmentation Tasks, TechBeat Talk
                </li>
            </ul>
        </div>
    </div>



    <div class="section">
        <h2>Academic Services</h2>
        <div class="paper">
            <strong>Workshop (Co-)Organizer</strong><br>
            <ul>
                <li>
                    Challenges and Opportunities of Large Models for CV/PR (大模型对CV/PR的挑战与机会) at VALSE 2023
                </li>
            </ul>


            <strong>Senior Program Committee Member</strong><br>
            <ul>
                <li>
                    International Joint Conference on Artificial Intelligence (IJCAI), 2021
                </li>
            </ul>

            <strong>Journal Reviewer</strong><br>
            <ul>
                <li>
                    IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
                </li>
                <li>
                    International Journal of Computer Vision (IJCV)
                </li>
                <li>
                    IEEE Transactions on Image Processing (TIP)
                </li>
                <li>
                    IEEE Transactions on Multimedia (TMM)
                </li>
                <li>
                    IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)
                </li>
                <li>
                    Computational Visual Media Journal (CVMJ)
                </li>
                <li>
                    Pattern Recognition (PR)
                </li>
            </ul>

            <strong>Program Committee Member/Conference Reviewer</strong><br>
            <ul>
                <li>
                    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, 2021, 2022, 2023
                </li>
                <li>
                    Neural Information Processing Systems (NeurIPS), 2020, 2021, 2023
                </li>
                <li>
                    International Conference on Machine Learning (ICML), 2021, 2022
                </li>
                <li>
                    International Conference on Learning Representations (ICLR), 2021
                </li>
                <li>
                    IEEE International Conference on Computer Vision (ICCV), 2021
                </li>
                <li>
                    European Conference on Computer Vision (ECCV), 2022
                </li>
                <li>
                    AAAI Conference on Artificial Intelligence (AAAI), 2022
                </li>
                <li>
                    International Joint Conference on Artificial Intelligence (IJCAI), 2022
                </li>
                <li>
                    IEEE Winter Conference on Applications of Computer Vision (WACV), 2021
                </li>
                <li>
                    Asian Conference on Computer Vision (ACCV), 2020
                </li>
            </ul>
        </div>
    </div>


    <div style='width:850px;height:300px;margin:0 auto'>
        <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
        <script type="text/javascript" id="clustrmaps"
            src="//cdn.clustrmaps.com/map_v2.js?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff&w=a"></script>
    </div>
</body>

</html>